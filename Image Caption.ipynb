{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook uses TensorFlow version 1.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "\n",
    "print(\"This notebook uses TensorFlow version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 26900 vocabularies\n"
     ]
    }
   ],
   "source": [
    "vocab = cPickle.load(open('src/vocab.pkl', 'rb'))\n",
    "print('total {} vocabularies'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count vocabulary occurances...\n",
      "3153 words appear >= 50 times\n"
     ]
    }
   ],
   "source": [
    "def count_vocab_occurance(vocab, df):\n",
    "    voc_cnt = {v: 0 for v in vocab}\n",
    "    for img_id, row in df.iterrows():\n",
    "        for w in row['caption'].split(' '):\n",
    "            voc_cnt[w] += 1\n",
    "    return voc_cnt\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(os.path.join('src', 'train.csv'))\n",
    "\n",
    "print('count vocabulary occurances...')\n",
    "voc_cnt = count_vocab_occurance(vocab, df_train)\n",
    "\n",
    "# remove words appear < 50 times\n",
    "thrhd = 50\n",
    "x = np.array(list(voc_cnt.values()))\n",
    "print('{} words appear >= 50 times'.format(np.sum(x[(-x).argsort()] >= thrhd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_voc_mapping(voc_cnt, thrhd):\n",
    "    \"\"\"\n",
    "    enc_map: voc --encode--> id\n",
    "    dec_map: id --decode--> voc\n",
    "    \"\"\"\n",
    "\n",
    "    def add(enc_map, dec_map, voc):\n",
    "        enc_map[voc] = len(dec_map)\n",
    "        dec_map[len(dec_map)] = voc\n",
    "        return enc_map, dec_map\n",
    "\n",
    "    # add <ST>, <ED>, <RARE>\n",
    "    enc_map, dec_map = {}, {}\n",
    "    for voc in ['<ST>', '<ED>', '<RARE>']:\n",
    "        enc_map, dec_map = add(enc_map, dec_map, voc)\n",
    "    for voc, cnt in voc_cnt.items():\n",
    "        if cnt < thrhd:  # rare words => <RARE>\n",
    "            enc_map[voc] = enc_map['<RARE>']\n",
    "        else:\n",
    "            enc_map, dec_map = add(enc_map, dec_map, voc)\n",
    "    return enc_map, dec_map\n",
    "\n",
    "\n",
    "enc_map, dec_map = build_voc_mapping(voc_cnt, thrhd)\n",
    "# save enc/decoding map to disk\n",
    "cPickle.dump(enc_map, open('src/enc_map.pkl', 'wb'))\n",
    "cPickle.dump(dec_map, open('src/dec_map.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[transform captions into sequences of IDs]...\n"
     ]
    }
   ],
   "source": [
    "def caption_to_ids(enc_map, df):\n",
    "    img_ids, caps = [], []\n",
    "    for idx, row in df.iterrows():\n",
    "        icap = [enc_map[x] for x in row['caption'].split(' ')]\n",
    "        icap.insert(0, enc_map['<ST>'])\n",
    "        icap.append(enc_map['<ED>'])\n",
    "        img_ids.append(row['img_id'])\n",
    "        caps.append(icap)\n",
    "    return pd.DataFrame({\n",
    "              'img_id': img_ids,\n",
    "              'caption': caps\n",
    "            }).set_index(['img_id'])\n",
    "\n",
    "\n",
    "enc_map = cPickle.load(open('src/enc_map.pkl', 'rb'))\n",
    "print('[transform captions into sequences of IDs]...')\n",
    "df_proc = caption_to_ids(enc_map, df_train)\n",
    "df_proc.to_csv('src/train_enc_cap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoding the encoded captions back...\n",
      "\n",
      "0: <ST> a group of three women sitting at a table sharing a cup of tea <ED>\n",
      "1: <ST> three women wearing hats at a table together <ED>\n",
      "2: <ST> three women with hats at a table having a tea party <ED>\n",
      "3: <ST> several woman dressed up with fancy hats at a tea party <ED>\n",
      "4: <ST> three women wearing large hats at a fancy tea event <ED>\n",
      "5: <ST> a twin door refrigerator in a kitchen next to cabinets <ED>\n",
      "6: <ST> a black refrigerator freezer sitting inside of a kitchen <ED>\n",
      "7: <ST> black refrigerator in messy kitchen of residential home <ED>\n"
     ]
    }
   ],
   "source": [
    "df_cap = pd.read_csv(\n",
    "    'src/train_enc_cap.csv')  # a dataframe - 'img_id', 'cpation'\n",
    "enc_map = cPickle.load(\n",
    "    open('src/enc_map.pkl', 'rb'))  # token => id\n",
    "dec_map = cPickle.load(\n",
    "    open('src/dec_map.pkl', 'rb'))  # id => token\n",
    "vocab_size = len(dec_map)\n",
    "\n",
    "\n",
    "def decode(dec_map, ids):\n",
    "    \"\"\"decode IDs back to origin caption string\"\"\"\n",
    "    return ' '.join([dec_map[x] for x in ids])\n",
    "\n",
    "\n",
    "print('decoding the encoded captions back...\\n')\n",
    "for idx, row in df_cap.iloc[:8].iterrows():\n",
    "    print('{}: {}'.format(idx, decode(dec_map, eval(row['caption']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images for training: 102739\n"
     ]
    }
   ],
   "source": [
    "img_train = cPickle.load(open('src/train_img256.pkl', 'rb'))\n",
    "# transform img_dict to dataframe\n",
    "img_train_df = pd.DataFrame(list(img_train.items()), columns=['img_id', 'img'])\n",
    "print('Images for training: {}'.format(img_train_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecords(df_cap, img_df, filename, num_files=5):\n",
    "    ''' create tfrecords for dataset '''\n",
    "\n",
    "    def _float_feature(value):\n",
    "        return tf.train.Feature(\n",
    "            float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "    def _int64_feature(value):\n",
    "        return tf.train.Feature(\n",
    "            int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "    num_records_per_file = img_df.shape[0] // num_files\n",
    "\n",
    "    total_count = 0\n",
    "\n",
    "    print(\"create training dataset....\")\n",
    "    for i in range(num_files):\n",
    "        # tfrecord writer: write record into files\n",
    "        count = 0\n",
    "        writer = tf.python_io.TFRecordWriter(\n",
    "            filename + '-' + str(i + 1) +'.tfrecords')\n",
    "        \n",
    "        # start point (inclusive)\n",
    "        st = i * num_records_per_file  \n",
    "        # end point (exclusive)\n",
    "        ed = (i + 1) * num_records_per_file if i != num_files - 1 else img_df.shape[0]  \n",
    "\n",
    "        for idx, row in img_df.iloc[st:ed].iterrows():\n",
    "        \n",
    "            # img representation in 256-d array format\n",
    "            img_representation = row['img']  \n",
    "\n",
    "            # each image has some captions describing it.\n",
    "            for _, inner_row in df_cap[df_cap['img_id'] == row['img_id']].iterrows():\n",
    "                # caption in different sequence length list format\n",
    "                caption = eval(inner_row['caption'])  \n",
    "\n",
    "                # construct 'example' object containing 'img', 'caption'\n",
    "                example = tf.train.Example(features=tf.train.Features(\n",
    "                    feature={\n",
    "                        'img': _float_feature(img_representation),\n",
    "                        'caption': _int64_feature(caption)\n",
    "                    }))\n",
    "\n",
    "                count += 1\n",
    "                writer.write(example.SerializeToString())\n",
    "        print(\"create {}-{}.tfrecords -- contains {} records\".format(\n",
    "                                    filename, str(i + 1), count))\n",
    "        total_count += count\n",
    "        writer.close()\n",
    "    print(\"Total records: {}\".format(total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create training dataset....\n",
      "create src/tfrecords/train-1.tfrecords -- contains 51385 records\n",
      "create src/tfrecords/train-2.tfrecords -- contains 51399 records\n",
      "create src/tfrecords/train-3.tfrecords -- contains 51395 records\n",
      "create src/tfrecords/train-4.tfrecords -- contains 51389 records\n",
      "create src/tfrecords/train-5.tfrecords -- contains 51390 records\n",
      "create src/tfrecords/train-6.tfrecords -- contains 51397 records\n",
      "create src/tfrecords/train-7.tfrecords -- contains 51397 records\n",
      "create src/tfrecords/train-8.tfrecords -- contains 51388 records\n",
      "create src/tfrecords/train-9.tfrecords -- contains 51387 records\n",
      "create src/tfrecords/train-10.tfrecords -- contains 51442 records\n",
      "Total records: 513969\n"
     ]
    }
   ],
   "source": [
    "# uncomment next line to create tfrecords file\n",
    "# create_tfrecords(df_cap, img_train_df, 'src/tfrecords/train', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training records in all training file: 513969\n",
      "src/tfrecords/train-2.tfrecords\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "training_filenames = glob.glob('src/tfrecords/train-*')\n",
    "\n",
    "# get the number of records in training files\n",
    "def get_num_records(files):\n",
    "    count = 0\n",
    "    for fn in files:\n",
    "        for record in tf.python_io.tf_record_iterator(fn):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "num_train_records = get_num_records(training_filenames)\n",
    "print('Number of training records in all training file: {}'.format(\n",
    "    num_train_records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_parser(record):\n",
    "    ''' parse record from .tfrecords file and create training record\n",
    "\n",
    "    :args \n",
    "      record - each record extracted from .tfrecords\n",
    "    :return\n",
    "      a dictionary contains {\n",
    "          'img': image array extracted from vgg16 (256-dim),\n",
    "          'input_seq': a list of word id\n",
    "                    which describes input caption sequence (Tensor),\n",
    "          'output_seq': a list of word id\n",
    "                    which describes output caption sequence (Tensor),\n",
    "          'mask': a list of one which describe\n",
    "                    the length of input caption sequence (Tensor)\n",
    "      }\n",
    "    '''\n",
    "\n",
    "    keys_to_features = {\n",
    "      \"img\": tf.FixedLenFeature([256], dtype=tf.float32),\n",
    "      \"caption\": tf.VarLenFeature(dtype=tf.int64)\n",
    "    }\n",
    "\n",
    "    # features contains - 'img', 'caption'\n",
    "    features = tf.parse_single_example(record, features=keys_to_features)\n",
    "\n",
    "    img = features['img']\n",
    "    caption = features['caption'].values\n",
    "    caption = tf.cast(caption, tf.int32)\n",
    "\n",
    "    # create input and output sequence for each training example\n",
    "    # e.g. caption :   [0 2 5 7 9 1]\n",
    "    #      input_seq:  [0 2 5 7 9]\n",
    "    #      output_seq: [2 5 7 9 1]\n",
    "    #      mask:       [1 1 1 1 1]\n",
    "    caption_len = tf.shape(caption)[0]\n",
    "    input_len = tf.expand_dims(tf.subtract(caption_len, 1), 0)\n",
    "\n",
    "    input_seq = tf.slice(caption, [0], input_len)\n",
    "    output_seq = tf.slice(caption, [1], input_len)\n",
    "    mask = tf.ones(input_len, dtype=tf.int32)\n",
    "\n",
    "    records = {\n",
    "      'img': img,\n",
    "      'input_seq': input_seq,\n",
    "      'output_seq': output_seq,\n",
    "      'mask': mask\n",
    "    }\n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecord_iterator(filenames, batch_size, record_parser):\n",
    "    ''' create iterator to eat tfrecord dataset \n",
    "\n",
    "    :args\n",
    "        filenames     - a list of filenames (string)\n",
    "        batch_size    - batch size (positive int)\n",
    "        record_parser - a parser that read tfrecord\n",
    "                        and create example record (function)\n",
    "\n",
    "    :return \n",
    "        iterator      - an Iterator providing a way\n",
    "                        to extract elements from the created dataset.\n",
    "        output_types  - the output types of the created dataset.\n",
    "        output_shapes - the output shapes of the created dataset.\n",
    "    '''\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(record_parser, num_parallel_calls=16)\n",
    "\n",
    "    # padded into equal length in each batch\n",
    "    dataset = dataset.padded_batch(\n",
    "      batch_size=batch_size,\n",
    "      padded_shapes={\n",
    "          'img': [None],\n",
    "          'input_seq': [None],\n",
    "          'output_seq': [None],\n",
    "          'mask': [None]\n",
    "      },\n",
    "      padding_values={\n",
    "          'img': 1.0,       # needless, for completeness\n",
    "          'input_seq': 1,   # padding input sequence in this batch\n",
    "          'output_seq': 1,  # padding output sequence in this batch\n",
    "          'mask': 0         # padding 0 means no words in this position\n",
    "      })  \n",
    "\n",
    "    dataset = dataset.repeat()             # repeat dataset infinitely\n",
    "    dataset = dataset.shuffle(3*batch_size)  # shuffle the dataset\n",
    "\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    output_types = dataset.output_types\n",
    "    output_shapes = dataset.output_shapes\n",
    "\n",
    "    return iterator, output_types, output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionModel(object):\n",
    "    ''' simple image caption model '''\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        self.hps = hparams\n",
    "\n",
    "    def _build_inputs(self):\n",
    "        \"\"\" construct the inputs for model \"\"\"\n",
    "        self.filenames = tf.placeholder(tf.string,\n",
    "                                        shape=[None], name='filenames')\n",
    "        self.training_iterator, types, shapes = tfrecord_iterator(\n",
    "          self.filenames, self.hps.batch_size, training_parser)\n",
    "\n",
    "        self.handle = tf.placeholder(tf.string, shape=[], name='handle')\n",
    "        iterator = tf.data.Iterator.from_string_handle(self.handle,\n",
    "                                                       types, shapes)\n",
    "        records = iterator.get_next()\n",
    "\n",
    "        image_embed = records['img']\n",
    "        image_embed.set_shape([None, self.hps.image_embedding_size])\n",
    "        input_seq = records['input_seq']\n",
    "        target_seq = records['output_seq']\n",
    "        input_mask = records['mask']\n",
    "        \n",
    "        self.image_embed = image_embed # (batch_size, img_dim)\n",
    "        self.input_seq = input_seq # (batch_size, seqlen)\n",
    "        self.target_seq = target_seq # (batch_size, seqlen)\n",
    "        self.input_mask = input_mask # (batch_size, seqlen)\n",
    "            \n",
    "        # convert sequence of index to sequence of embedding\n",
    "        with tf.variable_scope('seq_embedding'), tf.device('/cpu:0'):\n",
    "            self.embedding_matrix = tf.get_variable(\n",
    "                    name='embedding_matrix',\n",
    "                    shape=[self.hps.vocab_size,\n",
    "                           self.hps.word_embedding_size],\n",
    "                    initializer=tf.random_uniform_initializer(\n",
    "                        minval=-1, maxval=1))\n",
    "            # [batch_size, seqlen, embedding_size]\n",
    "            seq_embeddings = tf.nn.embedding_lookup(\n",
    "                self.embedding_matrix, self.input_seq)\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \"\"\" Build your image caption model \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def build(self):\n",
    "        \"\"\" call this function to build the inputs and model \"\"\"\n",
    "        self._build_inputs()\n",
    "        self._build_model()\n",
    "        \n",
    "    def train(self, sess, training_filenames, num_train_records):\n",
    "        \"\"\" write a training function for your model \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, sess, img_vec, dec_map):\n",
    "        \"\"\" generate the caption given an image \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hparams():\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "      vocab_size=vocab_size,\n",
    "      batch_size=64,\n",
    "      rnn_units=100,\n",
    "      image_embedding_size=256,\n",
    "      word_embedding_size=256,\n",
    "      drop_keep_prob=0.7,\n",
    "      lr=1e-3,\n",
    "      training_epochs=1,\n",
    "      max_caption_len=15,\n",
    "      ckpt_dir='model_ckpt/')\n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hperparameters\n",
    "hparams = get_hparams()\n",
    "# create model\n",
    "model = ImageCaptionModel(hparams)\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "model.train(sess, training_filenames, num_train_records)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
